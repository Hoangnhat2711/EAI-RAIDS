"""
Demo sá»­ dá»¥ng Responsible AI Framework

VÃ­ dá»¥ nÃ y minh há»a cÃ¡ch sá»­ dá»¥ng framework Ä‘á»ƒ:
1. Train má»™t model vá»›i cÃ¡c kiá»ƒm tra trÃ¡ch nhiá»‡m
2. ÄÃ¡nh giÃ¡ fairness
3. Giáº£i thÃ­ch predictions
4. Audit logging
5. Kiá»ƒm tra compliance
"""

import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Import Responsible AI Framework
from core.responsible_ai import ResponsibleAI
from core.model_wrapper import ResponsibleModelWrapper
from core.validator import ComplianceValidator
from fairness.metrics import FairnessMetrics
from fairness.bias_detector import BiasDetector
from audit.logger import AuditLogger
from audit.reporter import AuditReporter


def create_sample_data():
    """Táº¡o sample data cho demo"""
    print("\nğŸ“Š Táº¡o sample dataset...")
    
    # Táº¡o synthetic dataset
    X, y = make_classification(
        n_samples=1000,
        n_features=10,
        n_informative=8,
        n_redundant=2,
        random_state=42
    )
    
    # Táº¡o sensitive feature (giáº£ láº­p gender: 0 hoáº·c 1)
    sensitive_features = np.random.randint(0, 2, size=1000)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    sensitive_train = sensitive_features[:800]
    sensitive_test = sensitive_features[800:]
    
    print(f"âœ“ Dataset táº¡o xong: {len(X_train)} training samples, {len(X_test)} test samples")
    
    return X_train, X_test, y_train, y_test, sensitive_train, sensitive_test


def main():
    """Main demo function"""
    
    print("=" * 70)
    print("ğŸš€ DEMO: RESPONSIBLE AI FRAMEWORK")
    print("=" * 70)
    
    # 1. Khá»Ÿi táº¡o Responsible AI Framework
    print("\nğŸ“‹ BÆ°á»›c 1: Khá»Ÿi táº¡o Responsible AI Framework")
    rai = ResponsibleAI(config_path='config.yaml')
    print(f"âœ“ Framework khá»Ÿi táº¡o vá»›i cÃ¡c nguyÃªn táº¯c: {', '.join(rai.get_active_principles())}")
    
    # 2. Táº¡o data
    X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = create_sample_data()
    
    # 3. Khá»Ÿi táº¡o Audit Logger
    print("\nğŸ“ BÆ°á»›c 2: Khá»Ÿi táº¡o Audit Logger")
    audit_logger = AuditLogger(log_dir='audit_logs', responsible_ai_framework=rai)
    print(f"âœ“ Audit Logger sáºµn sÃ ng (Session: {audit_logger.session_id})")
    
    # 4. Kiá»ƒm tra bias trong data
    print("\nğŸ” BÆ°á»›c 3: Kiá»ƒm tra bias trong training data")
    bias_detector = BiasDetector(rai)
    data_bias_report = bias_detector.detect_data_bias(
        X_train, y_train, sensitive_train
    )
    
    if data_bias_report['bias_detected']:
        print(f"âš  PhÃ¡t hiá»‡n {len(data_bias_report['biases'])} loáº¡i bias trong data!")
        for bias in data_bias_report['biases']:
            print(f"  - {bias['type']} (Severity: {bias['severity']})")
    else:
        print("âœ“ KhÃ´ng phÃ¡t hiá»‡n bias nghiÃªm trá»ng trong training data")
    
    # Log bias detection
    audit_logger.log_bias_detection(
        bias_type='data_bias',
        detected=data_bias_report['bias_detected'],
        severity='medium' if data_bias_report['bias_detected'] else 'low',
        details=data_bias_report
    )
    
    # 5. Train model vá»›i Responsible AI wrapper
    print("\nğŸ¤– BÆ°á»›c 4: Training model vá»›i Responsible AI wrapper")
    
    # Táº¡o base model
    base_model = RandomForestClassifier(n_estimators=100, random_state=42)
    
    # Wrap vá»›i ResponsibleModelWrapper
    responsible_model = ResponsibleModelWrapper(base_model, rai)
    
    # Train
    responsible_model.fit(X_train, y_train, sensitive_features=sensitive_train)
    print("âœ“ Model training hoÃ n táº¥t!")
    
    # Log training
    audit_logger.log_training(
        model_type='RandomForestClassifier',
        dataset_info={
            'n_samples': len(X_train),
            'n_features': X_train.shape[1]
        },
        hyperparameters={'n_estimators': 100},
        metrics={'trained': True}
    )
    
    # 6. Predictions
    print("\nğŸ¯ BÆ°á»›c 5: Making predictions")
    y_pred = responsible_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"âœ“ Test Accuracy: {accuracy:.4f}")
    
    # Log predictions
    audit_logger.log_prediction(
        input_data=X_test,
        prediction=y_pred,
        model_info={'type': 'RandomForestClassifier'},
        confidence=accuracy
    )
    
    # 7. ÄÃ¡nh giÃ¡ Fairness
    print("\nâš–ï¸ BÆ°á»›c 6: ÄÃ¡nh giÃ¡ Fairness")
    fairness_results = responsible_model.evaluate_fairness(
        X_test, y_test, y_pred, sensitive_test
    )
    
    print("Fairness Metrics:")
    for metric, value in fairness_results.items():
        icon = "âœ“" if value >= 0.8 else "âš "
        print(f"  {icon} {metric}: {value:.4f}")
    
    # Log fairness check
    fairness_passed = all(v >= 0.8 for v in fairness_results.values())
    audit_logger.log_fairness_check(
        metrics=fairness_results,
        passed=fairness_passed,
        threshold=0.8
    )
    
    # 8. Kiá»ƒm tra prediction bias
    print("\nğŸ” BÆ°á»›c 7: Kiá»ƒm tra bias trong predictions")
    prediction_bias = bias_detector.detect_prediction_bias(
        y_test, y_pred, sensitive_test
    )
    
    if prediction_bias['bias_detected']:
        print(f"âš  PhÃ¡t hiá»‡n bias trong predictions!")
        for bias in prediction_bias['biases']:
            print(f"  - {bias['type']}")
    else:
        print("âœ“ KhÃ´ng phÃ¡t hiá»‡n bias trong predictions")
    
    # 9. Giáº£i thÃ­ch predictions (simplified - khÃ´ng require SHAP/LIME install)
    print("\nğŸ’¡ BÆ°á»›c 8: Model Explainability")
    print("âœ“ Framework há»— trá»£ SHAP vÃ  LIME explanations")
    print("  (Cáº§n cÃ i Ä‘áº·t: pip install shap lime)")
    
    # 10. Compliance Validation
    print("\nâœ… BÆ°á»›c 9: Kiá»ƒm tra Compliance")
    validator = ComplianceValidator(rai)
    compliance_results = validator.validate_all(
        responsible_model, X_test, y_test, sensitive_test
    )
    
    overall_status = "âœ“ Äáº T" if compliance_results['overall_compliance'] else "âœ— KHÃ”NG Äáº T"
    print(f"Tráº¡ng thÃ¡i tá»•ng thá»ƒ: {overall_status}")
    
    print("\nChi tiáº¿t:")
    for principle, result in compliance_results['checks'].items():
        status = "âœ“" if result['passed'] else "âœ—"
        print(f"  {status} {principle.capitalize()}: Score {result['score']:.2f}")
    
    # Äá» xuáº¥t cáº£i thiá»‡n
    recommendations = validator.get_recommendations()
    if recommendations:
        print("\nğŸ“Œ Äá» xuáº¥t cáº£i thiá»‡n:")
        for rec in recommendations[:3]:
            print(f"  â€¢ {rec}")
    
    # 11. Táº¡o Audit Reports
    print("\nğŸ“Š BÆ°á»›c 10: Táº¡o Audit Reports")
    reporter = AuditReporter(audit_logger)
    
    # Compliance report
    print("\n" + reporter.generate_compliance_report())
    
    # Activity report
    print("\n" + reporter.generate_activity_report())
    
    # 12. Táº¡o Model Responsibility Report
    print("\nğŸ“„ BÆ°á»›c 11: Model Responsibility Report")
    print(responsible_model.generate_responsibility_report(
        X_test, y_test, sensitive_test
    ))
    
    # 13. Framework Summary
    print("\nğŸ“‹ Framework Summary")
    print(rai.generate_summary_report())
    
    # Export reports
    print("\nğŸ’¾ Xuáº¥t bÃ¡o cÃ¡o...")
    reporter.export_report('full', 'audit_report_full.txt')
    print("âœ“ BÃ¡o cÃ¡o Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: audit_report_full.txt")
    
    print("\n" + "=" * 70)
    print("âœ¨ DEMO HOÃ€N Táº¤T!")
    print("=" * 70)
    print("\nğŸ“š CÃ¡c tÃ­nh nÄƒng Ä‘Ã£ demo:")
    print("  âœ“ Khá»Ÿi táº¡o Responsible AI Framework")
    print("  âœ“ Bias detection trong data vÃ  predictions")
    print("  âœ“ Training vá»›i responsible checks")
    print("  âœ“ Fairness evaluation")
    print("  âœ“ Compliance validation")
    print("  âœ“ Audit logging vÃ  reporting")
    print("  âœ“ Model explainability (framework support)")
    print("\nğŸ‰ Framework sáºµn sÃ ng Ä‘á»ƒ sá»­ dá»¥ng trong production!")
    print("=" * 70)


if __name__ == '__main__':
    main()

