# üìã EAI-RAIDS Quick Reference

**Version 3.1.0** | **Last Updated:** October 2025

---

## üöÄ Installation

```bash
# Basic installation
pip install -r requirements.txt

# Full installation (all features)
pip install numpy pandas scikit-learn \
    torch tensorflow \
    shap lime \
    mlflow dvc \
    dowhy causalml \
    opacus tensorflow-privacy
```

---

## ‚ö° 30-Second Quick Start

```python
from core import ResponsibleAI, ResponsibleModelWrapper
from sklearn.ensemble import RandomForestClassifier

# 1. Initialize
rai = ResponsibleAI()

# 2. Wrap your model
model = ResponsibleModelWrapper(RandomForestClassifier(), rai)

# 3. Train with automatic checks
model.fit(X_train, y_train, sensitive_features=sensitive_train)

# 4. Predict with fairness monitoring
predictions = model.predict(X_test)

# 5. Get full report
print(model.generate_responsibility_report(X_test, y_test, sensitive_test))
```

---

## üì¶ Module Quick Access

### Core Framework
```python
from core import ResponsibleAI, ResponsibleModelWrapper, ComplianceValidator
from core import DataConverter, ExperimentTracker
from core.adapters import GradientWrapper, OptimizationHelper
```

### Fairness (‚öñÔ∏è)
```python
from fairness import FairnessMetrics, BiasDetector
from fairness import AdversarialDebiasing, PrejudiceRemover, FairConstrainedOptimization
```

### Explainability (üí°)
```python
from explainability import SHAPExplainer, LIMEExplainer
from explainability import CounterfactualExplainer, CausalFeatureImportance
from explainability import DoWhyIntegration, CausalMLIntegration
```

### Privacy (üîí)
```python
from privacy import DifferentialPrivacy, DataAnonymizer
from privacy import DPSGDTrainer, OpacusIntegration, TensorFlowPrivacyIntegration
```

### Robustness (üõ°Ô∏è)
```python
from robustness import AttackGenerator, DefenseTrainer
from robustness import RandomizedSmoothing, IntervalBoundPropagation
from robustness import CertifiedRobustnessEvaluator
```

### Statistical Testing (üìä)
```python
from utils import SignificanceTest, NormalityTest, ModelComparison
from utils import MultipleComparisonCorrection, BootstrapCI
```

### MLOps (üîß)
```python
from core import MLflowIntegration, DVCIntegration, ExperimentTracker
from audit import AuditLogger
from monitoring import DriftDetector
```

---

## üéØ Common Tasks

### 1. Train Fair Model (In-Processing)

```python
from fairness import AdversarialDebiasing

# Method 1: Adversarial Debiasing
debiaser = AdversarialDebiasing(
    sensitive_attribute_idx=0,
    adversary_loss_weight=1.0,
    learning_rate=0.001,
    epochs=100
)
debiaser.fit(X_train, y_train)
predictions = debiaser.predict(X_test)

# Method 2: Prejudice Remover
from fairness import PrejudiceRemover

remover = PrejudiceRemover(
    sensitive_attribute_idx=0,
    eta=1.0,  # Regularization weight
    fairness_measure='mutual_info'
)
regularized_loss = remover.create_regularized_loss(base_loss_fn)
```

### 2. Certify Robustness

```python
from robustness import RandomizedSmoothing

# Initialize
smoother = RandomizedSmoothing(
    base_classifier=model,
    sigma=0.25,
    num_samples_certification=10000
)

# Get certified radius for each sample
predictions, radii = smoother.predict(X_test, return_radius=True)

# Certify at specific epsilon
results = smoother.certify(X_test, y_test, epsilon=0.5)
print(f"Certified accuracy: {results['certified_accuracy']:.2%}")
```

### 3. Causal Analysis

```python
from explainability import DoWhyIntegration

# Initialize
dowhy = DoWhyIntegration(
    treatment='education',
    outcome='income',
    common_causes=['age', 'gender', 'experience']
)

# Complete analysis
results = dowhy.complete_analysis(data_df)
print(f"Causal effect: {results['causal_effect']:.3f}")
print(f"Passed refutation: {results['robust']}")
```

### 4. Privacy-Preserving Training

```python
from privacy import OpacusIntegration

# Initialize
opacus = OpacusIntegration(
    epsilon=1.0,
    delta=1e-5,
    max_grad_norm=1.0
)

# Make model private
model, optimizer, privacy_engine = opacus.make_private(
    model, optimizer, train_loader, epochs=10
)

# Train with DP-SGD
history = opacus.train_private_model(
    model, optimizer, train_loader, criterion, epochs=10
)
```

### 5. Statistical Comparison

```python
from utils import ModelComparison

# Initialize with auto-assumption checking
comparator = ModelComparison(check_assumptions=True)

# Compare two models
results = comparator.compare_two_models(
    baseline_scores,
    improved_scores,
    model_a_name="Baseline",
    model_b_name="Our Method",
    paired=True,
    parametric=None  # Auto-detect!
)

print(results['conclusion'])
# Output: "Our Method significantly outperforms Baseline (p=0.0023)"
```

### 6. Experiment Tracking

```python
from core import ExperimentTracker

# Initialize
tracker = ExperimentTracker(
    "my-experiment",
    use_mlflow=True,
    use_dvc=True
)

# Start run
run_id = tracker.start_run(run_name="experiment-1")

# Log everything
tracker.log_params({'model': 'RandomForest', 'n_estimators': 100})
tracker.log_metrics({'accuracy': 0.95, 'f1': 0.93})
tracker.log_model(model)
tracker.log_responsible_ai_metrics({
    'fairness': fairness_results,
    'robustness': robustness_results
})

# End run
tracker.end_run()
```

---

## üî• Advanced Features

### Gradient Wrapper (10-3000x faster!)

```python
from core.adapters import GradientWrapper

# Wrap model for analytical gradients
grad_wrapper = GradientWrapper(model)

# Compute gradients
gradients = grad_wrapper.compute_gradients(X, y)

# Check if using analytical
print(grad_wrapper.is_analytical())  # True = fast!
```

### Counterfactual Explanations

```python
from explainability import CounterfactualExplainer

explainer = CounterfactualExplainer(
    model,
    use_analytical_gradients=True  # 50-100x faster!
)

cf = explainer.explain(
    X_instance,
    desired_class=1,
    features_to_vary=[2, 3, 4],  # Only vary these features
    max_iterations=1000
)

print(f"Original: {cf['original_prediction']}")
print(f"Counterfactual: {cf['counterfactual_prediction']}")
print(f"Changes needed: {cf['changes']}")
```

### Certified IBP for TensorFlow

```python
from robustness import IntervalBoundPropagation

# Now works for TensorFlow! (v3.1.0+)
ibp = IntervalBoundPropagation(tf_model, epsilon=0.1)
results = ibp.certify(X_test, y_test)

print(f"Method: {results['method']}")  # 'IBP (TensorFlow)'
print(f"Certified: {results['certified_accuracy']:.2%}")
```

---

## üéì Research Workflow

```python
# Complete research pipeline
from core import ExperimentTracker
from fairness import AdversarialDebiasing
from robustness import CertifiedRobustnessEvaluator
from utils import ModelComparison

# 1. Setup tracking
tracker = ExperimentTracker("paper-experiment")
tracker.start_run("main-results")

# 2. Train fair model
fair_model = AdversarialDebiasing(...)
fair_model.fit(X_train, y_train)

# 3. Certify robustness
cert_eval = CertifiedRobustnessEvaluator(fair_model, 'randomized_smoothing')
cert_results = cert_eval.evaluate(X_test, y_test, epsilon=0.5)

# 4. Statistical comparison
comparator = ModelComparison()
comparison = comparator.compare_two_models(
    baseline_scores, fair_scores, paired=True
)

# 5. Log everything
tracker.log_responsible_ai_metrics({
    'fairness': fairness_metrics,
    'certified_robustness': cert_results,
    'statistical_test': comparison
})

# 6. Generate report
tracker.end_run()
print(f"‚úÖ Results saved to MLflow: {tracker.current_run_id}")
```

---

## üêõ Troubleshooting

### Issue: Import errors

```python
# Try importing individually to locate issue
try:
    from core import ResponsibleAI
    print("‚úì Core works")
except ImportError as e:
    print(f"‚úó Core issue: {e}")
```

### Issue: Slow gradient computation

```python
# Check if using analytical gradients
from core.adapters import GradientWrapper

grad_wrapper = GradientWrapper(model)
print(f"Analytical: {grad_wrapper.is_analytical()}")
print(f"Info: {grad_wrapper.get_info()}")

# If False, ensure you're using proper adapters
```

### Issue: Causal inference errors

```python
# Ensure you're NOT falling back to correlation
from core import CausalInferenceError

try:
    selector.fit(data)
except CausalInferenceError as e:
    print(e)  # Will show installation instructions
    # Install: pip install causal-learn lingam
```

---

## üìö Documentation Map

| Document | Purpose |
|----------|---------|
| `README.md` | User guide & getting started |
| `EXECUTIVE_SUMMARY.md` | High-level overview for stakeholders |
| `SOTA_FEATURES.md` | Detailed SOTA feature documentation |
| `RESEARCH_RIGOR_FIXES.md` | Critical fixes & research integrity |
| `CRITICAL_FIXES.md` | Previous bug fixes |
| `PUBLICATION_READY.md` | Publication readiness status |
| `QUICK_REFERENCE.md` | This file - quick cheat sheet |
| `CITATION.bib` | BibTeX citations for papers |

---

## ‚ö° Performance Tips

1. **Use analytical gradients** - `GradientWrapper` is 10-3000x faster
2. **Batch processing** - RandomizedSmoothing uses batching automatically
3. **Parallel processing** - Enable multiprocessing for large datasets
4. **Cache results** - Use MLflow/DVC to avoid recomputation
5. **Profile code** - Use benchmarks to identify bottlenecks

---

## üîó Important Links

- **Repository:** https://github.com/Hoangnhat2711/EAI-RAIDS
- **Issues:** https://github.com/Hoangnhat2711/EAI-RAIDS/issues
- **Documentation:** All `*.md` files
- **Examples:** `examples/` directory
- **Tests:** `tests/` directory
- **Benchmarks:** `benchmarks/` directory

---

## üí° Pro Tips

1. **Start simple** - Use `examples/demo.py` first
2. **Check assumptions** - ModelComparison auto-checks normality
3. **Use wrappers** - ResponsibleModelWrapper adds automatic checks
4. **Track everything** - ExperimentTracker ensures reproducibility
5. **Test first** - Run unit tests before major changes

---

## üéØ Target Conferences

**Ready for submission:**
- ICML 2026 (Deadline: Jan 2026)
- NeurIPS 2025 (Deadline: May 2025)
- ICLR 2026 (Deadline: Sep 2025)
- AAAI 2026 (Deadline: Aug 2025)
- FAccT 2026 (Deadline: Jan 2026)
- CCS 2025 (Deadline: May 2025)
- MLSys 2026 (Deadline: Oct 2025)

---

**Need more help?** Check `examples/` or create an issue on GitHub!

**Version:** 3.1.0 | **License:** MIT | **Status:** Production-Ready

